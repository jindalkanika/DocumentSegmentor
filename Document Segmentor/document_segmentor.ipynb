{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test file path\n",
    "train_file = \"./dataset/train\"\n",
    "#input folder which needs to be segregated\n",
    "input_path = \"./dataset/test\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/train/entertainment\n",
      "./dataset/train/politics\n",
      "./dataset/train/business\n",
      "./dataset/train/sport\n",
      "./dataset/train/tech\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "./dataset/train/tech             10\n",
       "./dataset/train/entertainment    10\n",
       "./dataset/train/business         10\n",
       "./dataset/train/sport            10\n",
       "./dataset/train/politics         10\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########TRAINING DATASET PREPARATION#############################\n",
    "\n",
    "file_path = train_file\n",
    "for files in glob.glob(file_path + \"/*\", recursive=True):\n",
    "\n",
    "    print(files)\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "list_file = []\n",
    "type_file = []\n",
    "content_file = []\n",
    "\n",
    "for f in glob.glob(file_path + \"/*\", recursive=True):\n",
    "\n",
    "    for files in glob.glob(f + \"/*\" + \".\" + \"txt\", recursive=True):\n",
    "        file = open(files, 'r')\n",
    "        content = file.read()\n",
    "\n",
    "        list_file.append(files)\n",
    "        type_file.append(f.split('\\\\')[-1])\n",
    "        content_file.append(content)\n",
    "\n",
    "dataset['File Names'] = list_file\n",
    "dataset['Category'] = type_file\n",
    "dataset['Content'] = content_file\n",
    "dataset['category_id'] = dataset['Category'].factorize()[0]\n",
    "\n",
    "category_id_dataset = dataset[['Category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "\n",
    "dataset['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Names</th>\n",
       "      <th>Category</th>\n",
       "      <th>Content</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./dataset/train/entertainment/021.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Obituary: Dame Alicia Markova\\n\\nDame Alicia M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./dataset/train/entertainment/035.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Hundreds vie for best film Oscar\\n\\nA total of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./dataset/train/entertainment/043.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Stars pay tribute to actor Davis\\n\\nHollywood ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./dataset/train/entertainment/049.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>De Niro film leads US box office\\n\\nFilm star ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./dataset/train/entertainment/023.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Famed music director Viotti dies\\n\\nConductor ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./dataset/train/entertainment/042.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Berlin honours S Korean director\\n\\nSouth Kore...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./dataset/train/entertainment/039.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Aviator 'creator' in Oscars snub\\n\\nThe man wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./dataset/train/entertainment/022.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Fears raised over ballet future\\n\\nFewer child...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./dataset/train/entertainment/048.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>'Landmark movies' of 2004 hailed\\n\\nUS film pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./dataset/train/entertainment/050.txt</td>\n",
       "      <td>./dataset/train/entertainment</td>\n",
       "      <td>Willis sues over movie 'injury'\\n\\nActor Bruce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./dataset/train/politics/101.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Council tax rise 'reasonable'\\n\\nWelsh council...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./dataset/train/politics/011.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Mrs Howard gets key election role\\n\\nMichael H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./dataset/train/politics/007.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Fox attacks Blair's Tory 'lies'\\n\\nTony Blair ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./dataset/train/politics/004.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Labour chooses Manchester\\n\\nThe Labour Party ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./dataset/train/politics/010.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Crucial decision on super-casinos\\n\\nA decisio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./dataset/train/politics/009.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Campbell: E-mail row 'silly fuss'\\n\\nEx-No 10 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./dataset/train/politics/008.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Women MPs reveal sexist taunts\\n\\nWomen MPs en...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./dataset/train/politics/006.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>'Errors' doomed first Dome sale\\n\\nThe initial...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./dataset/train/politics/005.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Brown ally rejects Budget spree\\n\\nChancellor ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./dataset/train/politics/210.txt</td>\n",
       "      <td>./dataset/train/politics</td>\n",
       "      <td>Thousands join strike in Wales\\n\\nThousands of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>./dataset/train/business/021.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Rank 'set to sell off film unit'\\n\\nLeisure gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>./dataset/train/business/004.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>./dataset/train/business/023.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Mixed signals from French economy\\n\\nThe Frenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>./dataset/train/business/003.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>./dataset/train/business/019.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>India widens access to telecoms\\n\\nIndia has r...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>./dataset/train/business/008.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>India calls for fair trade rules\\n\\nIndia, whi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>./dataset/train/business/006.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Japan narrowly escapes recession\\n\\nJapan's ec...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>./dataset/train/business/020.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Call centre users 'lose patience'\\n\\nCustomers...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>./dataset/train/business/002.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>./dataset/train/business/025.txt</td>\n",
       "      <td>./dataset/train/business</td>\n",
       "      <td>Yukos loses US bankruptcy battle\\n\\nA judge ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>./dataset/train/sport/279.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Newcastle line up Babayaro\\n\\nNewcastle manage...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>./dataset/train/sport/267.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Robertson out to retain Euro lure\\n\\nHearts ma...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>./dataset/train/sport/289.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Fuming Robinson blasts officials\\n\\nEngland co...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>./dataset/train/sport/265.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Gronkjaer agrees switch to Madrid\\n\\nJesper Gr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>./dataset/train/sport/278.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Robben and Cole earn Chelsea win\\n\\nCheslea sa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>./dataset/train/sport/292.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Wales coach elated with win\\n\\nMike Ruddock pa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>./dataset/train/sport/263.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>McLeish ready for criticism\\n\\nRangers manager...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>./dataset/train/sport/266.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Benitez 'to launch Morientes bid'\\n\\nLiverpool...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>./dataset/train/sport/264.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>O'Leary agrees new Villa contract\\n\\nAston Vil...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>./dataset/train/sport/262.txt</td>\n",
       "      <td>./dataset/train/sport</td>\n",
       "      <td>Spurs to sign Iceland U21 star\\n\\nTottenham ar...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>./dataset/train/tech/391.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Peer-to-peer nets 'here to stay'\\n\\nPeer-to-pe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>./dataset/train/tech/395.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Cebit fever takes over Hanover\\n\\nThousands of...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>./dataset/train/tech/388.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Camera phones are 'must-haves'\\n\\nFour times m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>./dataset/train/tech/394.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>TV's future down the phone line\\n\\nInternet TV...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>./dataset/train/tech/387.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Progress on new internet domains\\n\\nBy early 2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>./dataset/train/tech/393.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Savvy searchers fail to spot ads\\n\\nInternet s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>./dataset/train/tech/390.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Anti-spam laws bite spammer hard\\n\\nThe net's ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>./dataset/train/tech/396.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>New consoles promise big problems\\n\\nMaking ga...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>./dataset/train/tech/389.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Mobile multimedia slow to catch on\\n\\nThere is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>./dataset/train/tech/392.txt</td>\n",
       "      <td>./dataset/train/tech</td>\n",
       "      <td>Broadband fuels online expression\\n\\nFast web ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               File Names                       Category  \\\n",
       "0   ./dataset/train/entertainment/021.txt  ./dataset/train/entertainment   \n",
       "1   ./dataset/train/entertainment/035.txt  ./dataset/train/entertainment   \n",
       "2   ./dataset/train/entertainment/043.txt  ./dataset/train/entertainment   \n",
       "3   ./dataset/train/entertainment/049.txt  ./dataset/train/entertainment   \n",
       "4   ./dataset/train/entertainment/023.txt  ./dataset/train/entertainment   \n",
       "5   ./dataset/train/entertainment/042.txt  ./dataset/train/entertainment   \n",
       "6   ./dataset/train/entertainment/039.txt  ./dataset/train/entertainment   \n",
       "7   ./dataset/train/entertainment/022.txt  ./dataset/train/entertainment   \n",
       "8   ./dataset/train/entertainment/048.txt  ./dataset/train/entertainment   \n",
       "9   ./dataset/train/entertainment/050.txt  ./dataset/train/entertainment   \n",
       "10       ./dataset/train/politics/101.txt       ./dataset/train/politics   \n",
       "11       ./dataset/train/politics/011.txt       ./dataset/train/politics   \n",
       "12       ./dataset/train/politics/007.txt       ./dataset/train/politics   \n",
       "13       ./dataset/train/politics/004.txt       ./dataset/train/politics   \n",
       "14       ./dataset/train/politics/010.txt       ./dataset/train/politics   \n",
       "15       ./dataset/train/politics/009.txt       ./dataset/train/politics   \n",
       "16       ./dataset/train/politics/008.txt       ./dataset/train/politics   \n",
       "17       ./dataset/train/politics/006.txt       ./dataset/train/politics   \n",
       "18       ./dataset/train/politics/005.txt       ./dataset/train/politics   \n",
       "19       ./dataset/train/politics/210.txt       ./dataset/train/politics   \n",
       "20       ./dataset/train/business/021.txt       ./dataset/train/business   \n",
       "21       ./dataset/train/business/004.txt       ./dataset/train/business   \n",
       "22       ./dataset/train/business/023.txt       ./dataset/train/business   \n",
       "23       ./dataset/train/business/003.txt       ./dataset/train/business   \n",
       "24       ./dataset/train/business/019.txt       ./dataset/train/business   \n",
       "25       ./dataset/train/business/008.txt       ./dataset/train/business   \n",
       "26       ./dataset/train/business/006.txt       ./dataset/train/business   \n",
       "27       ./dataset/train/business/020.txt       ./dataset/train/business   \n",
       "28       ./dataset/train/business/002.txt       ./dataset/train/business   \n",
       "29       ./dataset/train/business/025.txt       ./dataset/train/business   \n",
       "30          ./dataset/train/sport/279.txt          ./dataset/train/sport   \n",
       "31          ./dataset/train/sport/267.txt          ./dataset/train/sport   \n",
       "32          ./dataset/train/sport/289.txt          ./dataset/train/sport   \n",
       "33          ./dataset/train/sport/265.txt          ./dataset/train/sport   \n",
       "34          ./dataset/train/sport/278.txt          ./dataset/train/sport   \n",
       "35          ./dataset/train/sport/292.txt          ./dataset/train/sport   \n",
       "36          ./dataset/train/sport/263.txt          ./dataset/train/sport   \n",
       "37          ./dataset/train/sport/266.txt          ./dataset/train/sport   \n",
       "38          ./dataset/train/sport/264.txt          ./dataset/train/sport   \n",
       "39          ./dataset/train/sport/262.txt          ./dataset/train/sport   \n",
       "40           ./dataset/train/tech/391.txt           ./dataset/train/tech   \n",
       "41           ./dataset/train/tech/395.txt           ./dataset/train/tech   \n",
       "42           ./dataset/train/tech/388.txt           ./dataset/train/tech   \n",
       "43           ./dataset/train/tech/394.txt           ./dataset/train/tech   \n",
       "44           ./dataset/train/tech/387.txt           ./dataset/train/tech   \n",
       "45           ./dataset/train/tech/393.txt           ./dataset/train/tech   \n",
       "46           ./dataset/train/tech/390.txt           ./dataset/train/tech   \n",
       "47           ./dataset/train/tech/396.txt           ./dataset/train/tech   \n",
       "48           ./dataset/train/tech/389.txt           ./dataset/train/tech   \n",
       "49           ./dataset/train/tech/392.txt           ./dataset/train/tech   \n",
       "\n",
       "                                              Content  category_id  \n",
       "0   Obituary: Dame Alicia Markova\\n\\nDame Alicia M...            0  \n",
       "1   Hundreds vie for best film Oscar\\n\\nA total of...            0  \n",
       "2   Stars pay tribute to actor Davis\\n\\nHollywood ...            0  \n",
       "3   De Niro film leads US box office\\n\\nFilm star ...            0  \n",
       "4   Famed music director Viotti dies\\n\\nConductor ...            0  \n",
       "5   Berlin honours S Korean director\\n\\nSouth Kore...            0  \n",
       "6   Aviator 'creator' in Oscars snub\\n\\nThe man wh...            0  \n",
       "7   Fears raised over ballet future\\n\\nFewer child...            0  \n",
       "8   'Landmark movies' of 2004 hailed\\n\\nUS film pr...            0  \n",
       "9   Willis sues over movie 'injury'\\n\\nActor Bruce...            0  \n",
       "10  Council tax rise 'reasonable'\\n\\nWelsh council...            1  \n",
       "11  Mrs Howard gets key election role\\n\\nMichael H...            1  \n",
       "12  Fox attacks Blair's Tory 'lies'\\n\\nTony Blair ...            1  \n",
       "13  Labour chooses Manchester\\n\\nThe Labour Party ...            1  \n",
       "14  Crucial decision on super-casinos\\n\\nA decisio...            1  \n",
       "15  Campbell: E-mail row 'silly fuss'\\n\\nEx-No 10 ...            1  \n",
       "16  Women MPs reveal sexist taunts\\n\\nWomen MPs en...            1  \n",
       "17  'Errors' doomed first Dome sale\\n\\nThe initial...            1  \n",
       "18  Brown ally rejects Budget spree\\n\\nChancellor ...            1  \n",
       "19  Thousands join strike in Wales\\n\\nThousands of...            1  \n",
       "20  Rank 'set to sell off film unit'\\n\\nLeisure gr...            2  \n",
       "21  High fuel prices hit BA's profits\\n\\nBritish A...            2  \n",
       "22  Mixed signals from French economy\\n\\nThe Frenc...            2  \n",
       "23  Yukos unit buyer faces loan claim\\n\\nThe owner...            2  \n",
       "24  India widens access to telecoms\\n\\nIndia has r...            2  \n",
       "25  India calls for fair trade rules\\n\\nIndia, whi...            2  \n",
       "26  Japan narrowly escapes recession\\n\\nJapan's ec...            2  \n",
       "27  Call centre users 'lose patience'\\n\\nCustomers...            2  \n",
       "28  Dollar gains on Greenspan speech\\n\\nThe dollar...            2  \n",
       "29  Yukos loses US bankruptcy battle\\n\\nA judge ha...            2  \n",
       "30  Newcastle line up Babayaro\\n\\nNewcastle manage...            3  \n",
       "31  Robertson out to retain Euro lure\\n\\nHearts ma...            3  \n",
       "32  Fuming Robinson blasts officials\\n\\nEngland co...            3  \n",
       "33  Gronkjaer agrees switch to Madrid\\n\\nJesper Gr...            3  \n",
       "34  Robben and Cole earn Chelsea win\\n\\nCheslea sa...            3  \n",
       "35  Wales coach elated with win\\n\\nMike Ruddock pa...            3  \n",
       "36  McLeish ready for criticism\\n\\nRangers manager...            3  \n",
       "37  Benitez 'to launch Morientes bid'\\n\\nLiverpool...            3  \n",
       "38  O'Leary agrees new Villa contract\\n\\nAston Vil...            3  \n",
       "39  Spurs to sign Iceland U21 star\\n\\nTottenham ar...            3  \n",
       "40  Peer-to-peer nets 'here to stay'\\n\\nPeer-to-pe...            4  \n",
       "41  Cebit fever takes over Hanover\\n\\nThousands of...            4  \n",
       "42  Camera phones are 'must-haves'\\n\\nFour times m...            4  \n",
       "43  TV's future down the phone line\\n\\nInternet TV...            4  \n",
       "44  Progress on new internet domains\\n\\nBy early 2...            4  \n",
       "45  Savvy searchers fail to spot ads\\n\\nInternet s...            4  \n",
       "46  Anti-spam laws bite spammer hard\\n\\nThe net's ...            4  \n",
       "47  New consoles promise big problems\\n\\nMaking ga...            4  \n",
       "48  Mobile multimedia slow to catch on\\n\\nThere is...            4  \n",
       "49  Broadband fuels online expression\\n\\nFast web ...            4  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1543)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################TF-IDF, FEATURE EXTRACTION AND LABEL######################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df = 2, norm='l2', encoding='latin-1', \n",
    "                        ngram_range=(1, 3), stop_words='english')\n",
    "features = tfidf.fit_transform(dataset.Content).toarray()\n",
    "labels = dataset.category_id\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.04325114, 0.        , 0.03961084, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# './dataset/train/business':\n",
      "  . Most correlated unigrams:\n",
      "       . economy\n",
      "       . friday\n",
      "       . unit\n",
      "       . yukos\n",
      "       . growth\n",
      "  . Most correlated bigrams:\n",
      "       . economic growth\n",
      "       . government hopes\n",
      "       . year march\n",
      "       . march 2005\n",
      "       . g7 meeting\n",
      "  . Most correlated trigrams:\n",
      "       . oil giant yukos\n",
      "       . forecast year march\n",
      "       . year march 2005\n",
      "       . major cost cutting\n",
      "       . longer term view\n",
      "# './dataset/train/entertainment':\n",
      "  . Most correlated unigrams:\n",
      "       . aviator\n",
      "       . oscar\n",
      "       . opera\n",
      "       . ballet\n",
      "       . film\n",
      "  . Most correlated bigrams:\n",
      "       . aviator million\n",
      "       . box office\n",
      "       . best film\n",
      "       . dollar baby\n",
      "       . million dollar\n",
      "  . Most correlated trigrams:\n",
      "       . director martin scorsese\n",
      "       . dollar baby sideways\n",
      "       . best film oscar\n",
      "       . aviator million dollar\n",
      "       . million dollar baby\n",
      "# './dataset/train/politics':\n",
      "  . Most correlated unigrams:\n",
      "       . tory\n",
      "       . public\n",
      "       . election\n",
      "       . party\n",
      "       . labour\n",
      "  . Most correlated bigrams:\n",
      "       . tory chairman\n",
      "       . prime minister\n",
      "       . election campaign\n",
      "       . mr blair\n",
      "       . said good\n",
      "  . Most correlated trigrams:\n",
      "       . million dollar baby\n",
      "       . chairman liam fox\n",
      "       . tory chairman liam\n",
      "       . chancellor gordon brown\n",
      "       . said good deal\n",
      "# './dataset/train/sport':\n",
      "  . Most correlated unigrams:\n",
      "       . uefa\n",
      "       . cup\n",
      "       . season\n",
      "       . contract\n",
      "       . chelsea\n",
      "  . Most correlated bigrams:\n",
      "       . transfer window\n",
      "       . according reports\n",
      "       . year old\n",
      "       . ve got\n",
      "       . uefa cup\n",
      "  . Most correlated trigrams:\n",
      "       . best film oscar\n",
      "       . told bbc news\n",
      "       . aviator million dollar\n",
      "       . million dollar baby\n",
      "       . told bbc radio\n",
      "# './dataset/train/tech':\n",
      "  . Most correlated unigrams:\n",
      "       . broadband\n",
      "       . phones\n",
      "       . online\n",
      "       . net\n",
      "       . microsoft\n",
      "  . Most correlated bigrams:\n",
      "       . mobile phone\n",
      "       . mobile phones\n",
      "       . camera phones\n",
      "       . anti spam\n",
      "       . net users\n",
      "  . Most correlated trigrams:\n",
      "       . told bbc radio\n",
      "       . best film oscar\n",
      "       . pleaded guilty charges\n",
      "       . aviator million dollar\n",
      "       . million dollar baby\n"
     ]
    }
   ],
   "source": [
    "########################N-GRAMS################################################\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "category_to_id = dict(category_id_dataset.values)\n",
    "id_to_category = dict(category_id_dataset[['category_id', 'Category']].values)\n",
    "N = 5\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "    print(\"# '{}':\".format(category))\n",
    "    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))\n",
    "    print(\"  . Most correlated trigrams:\\n       . {}\".format('\\n       . '.join(trigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################Logistic Regression MODEL TRAINING######################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, \n",
    "                                                                                 dataset.index, \n",
    "                                                                                 test_size=2, \n",
    "                                                                                 random_state=1)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=5, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################RANDOM FOREST CLASSIFIER TRAINING#############\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, \n",
    "                                                                                 dataset.index, \n",
    "                                                                                 test_size=2, \n",
    "                                                                                 random_state=1)\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################MULTIONOMIAL NAIVE BAYES MODEL TRAINING#############\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, \n",
    "                                                                                 dataset.index, \n",
    "                                                                                 test_size=2, \n",
    "                                                                                 random_state=7)\n",
    "multinomial_naive_bayes.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################TEST DATASET PREPARATION#########################\n",
    "\n",
    "test_file = input_path\n",
    "\n",
    "dataset_test = pd.DataFrame()\n",
    "list_file_test = []\n",
    "content_file_test = []\n",
    "\n",
    "for files in glob.glob(test_file + \"/*\" + \".\" + \"txt\", recursive=True):\n",
    "    file = open(files, 'r')\n",
    "    content = file.read()\n",
    "\n",
    "    list_file_test.append(files)\n",
    "    content_file_test.append(content)\n",
    "\n",
    "dataset_test['File Names'] = list_file_test\n",
    "dataset_test['Content'] = content_file_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Names</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./dataset/test/politics(2).txt</td>\n",
       "      <td>Howard's unfinished business\\n\\n\"He's not fini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./dataset/test/tech (8).txt</td>\n",
       "      <td>'Brainwave' cap controls computer\\n\\nA team of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./dataset/test/tech (15).txt</td>\n",
       "      <td>What price for 'trusted PC security'?\\n\\nYou c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./dataset/test/tech (16).txt</td>\n",
       "      <td>Text message record smashed again\\n\\nUK mobile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./dataset/test/business(3).txt</td>\n",
       "      <td>Worldcom boss 'left books alone'\\n\\nFormer Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./dataset/test/tech (6).txt</td>\n",
       "      <td>Looks and music to drive mobiles\\n\\nMobile pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./dataset/test/sport(1).txt</td>\n",
       "      <td>Newry to fight cup exit in courts\\n\\nNewry Cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./dataset/test/entertainment(1).txt</td>\n",
       "      <td>Rapper Snoop Dogg sued for 'rape'\\n\\nUS rapper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./dataset/test/tech (12).txt</td>\n",
       "      <td>Apple makes blogs reveal sources\\n\\nApple has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./dataset/test/sport(8).txt</td>\n",
       "      <td>Hansen 'delays return until 2006'\\n\\nBritish t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./dataset/test/tech (20).txt</td>\n",
       "      <td>Sun offers processing by the hour\\n\\nSun Micro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./dataset/test/tech (11).txt</td>\n",
       "      <td>Rivals of the Â£400 Apple...\\n\\nThe Mac mini is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./dataset/test/tech (17).txt</td>\n",
       "      <td>Commodore finds new lease of life\\n\\nThe once-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./dataset/test/tech (5).txt</td>\n",
       "      <td>Warnings about junk mail deluge\\n\\nThe amount ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./dataset/test/sport(3).txt</td>\n",
       "      <td>Jansen suffers a further setback\\n\\nBlackburn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./dataset/test/tech (10).txt</td>\n",
       "      <td>Fast lifts rise into record books\\n\\nTwo high-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./dataset/test/politics(5).txt</td>\n",
       "      <td>Labour MP praises Tory campaign\\n\\nThe Conserv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./dataset/test/tech (14).txt</td>\n",
       "      <td>Have hackers recruited your PC?\\n\\nMore than o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./dataset/test/sport(5).txt</td>\n",
       "      <td>Preview: Ireland v England (Sun)\\n\\nLansdowne ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./dataset/test/tech.txt</td>\n",
       "      <td>Moving mobile improves golf swing\\n\\nA mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>./dataset/test/sport(4).txt</td>\n",
       "      <td>Old Firm pair handed suspensions\\n\\nCeltic's H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>./dataset/test/tech (2).txt</td>\n",
       "      <td>Mobile music challenges 'iPod age'\\n\\nNokia an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>./dataset/test/politics(1).txt</td>\n",
       "      <td>Sayeed to stand down as Tory MP\\n\\nTory MP Jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>./dataset/test/tech (13).txt</td>\n",
       "      <td>Hitachi unveils 'fastest robot'\\n\\nJapanese el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>./dataset/test/politics(3).txt</td>\n",
       "      <td>'Last chance' warning for voters\\n\\nPeople in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>./dataset/test/entertainment(2).txt</td>\n",
       "      <td>Elton plays Paris charity concert\\n\\nSir Elton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>./dataset/test/politics(6).txt</td>\n",
       "      <td>Lib Dems unveil election slogan\\n\\nThe Liberal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>./dataset/test/tech (1).txt</td>\n",
       "      <td>Tough rules for ringtone sellers\\n\\nFirms that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>./dataset/test/politics(4).txt</td>\n",
       "      <td>Sainsbury's Labour election gift\\n\\nScience Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>./dataset/test/business(1).txt</td>\n",
       "      <td>Home loan approvals rising again\\n\\nThe number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>./dataset/test/tech (3).txt</td>\n",
       "      <td>Beckham virus spotted on the net\\n\\nVirus writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>./dataset/test/sport(2).txt</td>\n",
       "      <td>Souness delight at Euro progress\\n\\nBoss Graem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>./dataset/test/tech (9).txt</td>\n",
       "      <td>Britons growing 'digitally obese'\\n\\nGadget lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>./dataset/test/sport(9).txt</td>\n",
       "      <td>Off-colour Gardener storms to win\\n\\nBritain's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>./dataset/test/sport(7).txt</td>\n",
       "      <td>Johnson uncertain about Euro bid\\n\\nJade Johns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>./dataset/test/sport(11).txt</td>\n",
       "      <td>Radcliffe yet to answer GB call\\n\\nPaula Radcl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>./dataset/test/sport(6).txt</td>\n",
       "      <td>Calder fears for Scottish rugby\\n\\nFormer Scot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>./dataset/test/tech (19).txt</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n\\nThe Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>./dataset/test/business(2).txt</td>\n",
       "      <td>Manufacturing recovery 'slowing'\\n\\nUK manufac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>./dataset/test/tech (18).txt</td>\n",
       "      <td>T-Mobile bets on 'pocket office'\\n\\nT-Mobile h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>./dataset/test/tech (4).txt</td>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>./dataset/test/sport(10).txt</td>\n",
       "      <td>Collins to compete in Birmingham\\n\\nWorld and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>./dataset/test/entertainment(3).txt</td>\n",
       "      <td>Elvis set to top UK singles chart\\n\\nRock 'n' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>./dataset/test/tech (7).txt</td>\n",
       "      <td>Robots learn 'robotiquette' rules\\n\\nRobots ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>./dataset/test/entertainment(4).txt</td>\n",
       "      <td>Wal-Mart is sued over rude lyrics\\n\\nThe paren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>./dataset/test/entertainment(5).txt</td>\n",
       "      <td>Pete Doherty misses bail deadline\\n\\nSinger Pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             File Names  \\\n",
       "0        ./dataset/test/politics(2).txt   \n",
       "1           ./dataset/test/tech (8).txt   \n",
       "2          ./dataset/test/tech (15).txt   \n",
       "3          ./dataset/test/tech (16).txt   \n",
       "4        ./dataset/test/business(3).txt   \n",
       "5           ./dataset/test/tech (6).txt   \n",
       "6           ./dataset/test/sport(1).txt   \n",
       "7   ./dataset/test/entertainment(1).txt   \n",
       "8          ./dataset/test/tech (12).txt   \n",
       "9           ./dataset/test/sport(8).txt   \n",
       "10         ./dataset/test/tech (20).txt   \n",
       "11         ./dataset/test/tech (11).txt   \n",
       "12         ./dataset/test/tech (17).txt   \n",
       "13          ./dataset/test/tech (5).txt   \n",
       "14          ./dataset/test/sport(3).txt   \n",
       "15         ./dataset/test/tech (10).txt   \n",
       "16       ./dataset/test/politics(5).txt   \n",
       "17         ./dataset/test/tech (14).txt   \n",
       "18          ./dataset/test/sport(5).txt   \n",
       "19              ./dataset/test/tech.txt   \n",
       "20          ./dataset/test/sport(4).txt   \n",
       "21          ./dataset/test/tech (2).txt   \n",
       "22       ./dataset/test/politics(1).txt   \n",
       "23         ./dataset/test/tech (13).txt   \n",
       "24       ./dataset/test/politics(3).txt   \n",
       "25  ./dataset/test/entertainment(2).txt   \n",
       "26       ./dataset/test/politics(6).txt   \n",
       "27          ./dataset/test/tech (1).txt   \n",
       "28       ./dataset/test/politics(4).txt   \n",
       "29       ./dataset/test/business(1).txt   \n",
       "30          ./dataset/test/tech (3).txt   \n",
       "31          ./dataset/test/sport(2).txt   \n",
       "32          ./dataset/test/tech (9).txt   \n",
       "33          ./dataset/test/sport(9).txt   \n",
       "34          ./dataset/test/sport(7).txt   \n",
       "35         ./dataset/test/sport(11).txt   \n",
       "36          ./dataset/test/sport(6).txt   \n",
       "37         ./dataset/test/tech (19).txt   \n",
       "38       ./dataset/test/business(2).txt   \n",
       "39         ./dataset/test/tech (18).txt   \n",
       "40          ./dataset/test/tech (4).txt   \n",
       "41         ./dataset/test/sport(10).txt   \n",
       "42  ./dataset/test/entertainment(3).txt   \n",
       "43          ./dataset/test/tech (7).txt   \n",
       "44  ./dataset/test/entertainment(4).txt   \n",
       "45  ./dataset/test/entertainment(5).txt   \n",
       "\n",
       "                                              Content  \n",
       "0   Howard's unfinished business\\n\\n\"He's not fini...  \n",
       "1   'Brainwave' cap controls computer\\n\\nA team of...  \n",
       "2   What price for 'trusted PC security'?\\n\\nYou c...  \n",
       "3   Text message record smashed again\\n\\nUK mobile...  \n",
       "4   Worldcom boss 'left books alone'\\n\\nFormer Wor...  \n",
       "5   Looks and music to drive mobiles\\n\\nMobile pho...  \n",
       "6   Newry to fight cup exit in courts\\n\\nNewry Cit...  \n",
       "7   Rapper Snoop Dogg sued for 'rape'\\n\\nUS rapper...  \n",
       "8   Apple makes blogs reveal sources\\n\\nApple has ...  \n",
       "9   Hansen 'delays return until 2006'\\n\\nBritish t...  \n",
       "10  Sun offers processing by the hour\\n\\nSun Micro...  \n",
       "11  Rivals of the Â£400 Apple...\\n\\nThe Mac mini is...  \n",
       "12  Commodore finds new lease of life\\n\\nThe once-...  \n",
       "13  Warnings about junk mail deluge\\n\\nThe amount ...  \n",
       "14  Jansen suffers a further setback\\n\\nBlackburn ...  \n",
       "15  Fast lifts rise into record books\\n\\nTwo high-...  \n",
       "16  Labour MP praises Tory campaign\\n\\nThe Conserv...  \n",
       "17  Have hackers recruited your PC?\\n\\nMore than o...  \n",
       "18  Preview: Ireland v England (Sun)\\n\\nLansdowne ...  \n",
       "19  Moving mobile improves golf swing\\n\\nA mobile ...  \n",
       "20  Old Firm pair handed suspensions\\n\\nCeltic's H...  \n",
       "21  Mobile music challenges 'iPod age'\\n\\nNokia an...  \n",
       "22  Sayeed to stand down as Tory MP\\n\\nTory MP Jon...  \n",
       "23  Hitachi unveils 'fastest robot'\\n\\nJapanese el...  \n",
       "24  'Last chance' warning for voters\\n\\nPeople in ...  \n",
       "25  Elton plays Paris charity concert\\n\\nSir Elton...  \n",
       "26  Lib Dems unveil election slogan\\n\\nThe Liberal...  \n",
       "27  Tough rules for ringtone sellers\\n\\nFirms that...  \n",
       "28  Sainsbury's Labour election gift\\n\\nScience Mi...  \n",
       "29  Home loan approvals rising again\\n\\nThe number...  \n",
       "30  Beckham virus spotted on the net\\n\\nVirus writ...  \n",
       "31  Souness delight at Euro progress\\n\\nBoss Graem...  \n",
       "32  Britons growing 'digitally obese'\\n\\nGadget lo...  \n",
       "33  Off-colour Gardener storms to win\\n\\nBritain's...  \n",
       "34  Johnson uncertain about Euro bid\\n\\nJade Johns...  \n",
       "35  Radcliffe yet to answer GB call\\n\\nPaula Radcl...  \n",
       "36  Calder fears for Scottish rugby\\n\\nFormer Scot...  \n",
       "37  Apple laptop is 'greatest gadget'\\n\\nThe Apple...  \n",
       "38  Manufacturing recovery 'slowing'\\n\\nUK manufac...  \n",
       "39  T-Mobile bets on 'pocket office'\\n\\nT-Mobile h...  \n",
       "40  US cyber security chief resigns\\n\\nThe man mak...  \n",
       "41  Collins to compete in Birmingham\\n\\nWorld and ...  \n",
       "42  Elvis set to top UK singles chart\\n\\nRock 'n' ...  \n",
       "43  Robots learn 'robotiquette' rules\\n\\nRobots ar...  \n",
       "44  Wal-Mart is sued over rude lyrics\\n\\nThe paren...  \n",
       "45  Pete Doherty misses bail deadline\\n\\nSinger Pe...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.7391304347826086 for RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=5, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "Accuracy is 0.8695652173913043 for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy is 0.9565217391304348 for MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "test_features = tfidf.transform(dataset_test.Content).toarray()\n",
    "model_pipe = [random_forest,logistic_regression,multinomial_naive_bayes]\n",
    "for model in model_pipe:\n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    prediction_list = []\n",
    "    for predicted in (predictions):\n",
    "        type_file = id_to_category[predicted]\n",
    "        type_file = type_file.split('/')[-1]\n",
    "        prediction_list.append(type_file)\n",
    "\n",
    "    OriginalType = []\n",
    "    for i in dataset_test['File Names']:\n",
    "        file_name = i.split('/')[-1]\n",
    "        file_type = file_name.split('(')[0]\n",
    "        OriginalType.append(file_type)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    prediction_list = encoder.fit_transform(prediction_list)\n",
    "    OriginalType = encoder.fit_transform(OriginalType)\n",
    "    \n",
    "    accuracy = accuracy_score(prediction_list, OriginalType)\n",
    "\n",
    "    print(\"Accuracy is {} for {}\".format(accuracy,model))\n",
    "\n",
    "# dataset_test.to_excel(\"output_document_segmentation.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "test_features = tfidf.transform(dataset_test.Content).toarray()\n",
    "predictions = multinomial_naive_bayes.predict(test_features)\n",
    "\n",
    "prediction_list = []\n",
    "for predicted in (predictions):\n",
    "    type_file = id_to_category[predicted]\n",
    "    type_file = type_file.split('/')[-1]\n",
    "    prediction_list.append(type_file)\n",
    "\n",
    "dataset_test['Type'] = prediction_list\n",
    "\n",
    "OriginalType = []\n",
    "for i in dataset_test['File Names']:\n",
    "    file_name = i.split('/')[-1]\n",
    "    file_type = file_name.split('(')[0]\n",
    "    OriginalType.append(file_type)\n",
    "\n",
    "dataset_test['OriginalType'] = OriginalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ./sorted_files_16/politics\n",
      "Successfully created the directory ./sorted_files_16/tech\n",
      "Successfully created the directory ./sorted_files_16/business\n",
      "Successfully created the directory ./sorted_files_16/sport\n",
      "Successfully created the directory ./sorted_files_16/entertainment\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "file_number = str(random.randint(1,100))\n",
    "target_folder = \"./sorted_files_\" + file_number + \"/\"\n",
    "required_folders = dataset_test['Type'].unique().tolist()\n",
    "import os\n",
    "for i in required_folders:\n",
    "# define the name of the directory to be created\n",
    "    path = target_folder + i\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % path)\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    print (i)\n",
    "    source = dataset_test['File Names'][i]\n",
    "    destination =  target_folder + \"\" + dataset_test['Type'][i] \n",
    "    shutil.copy(source, destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
